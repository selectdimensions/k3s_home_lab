[![Kubernetes Applications CD](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/k8s-apps-cd.yml/badge.svg?branch=main)](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/k8s-apps-cd.yml)
[![Puppet CI/CD](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/puppet-ci.yml/badge.svg?branch=main)](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/puppet-ci.yml)
[![Security Scanning](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/security-scan.yml/badge.svg?branch=main)](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/security-scan.yml)
[![Terraform CI/CD](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/terraform-ci.yml/badge.svg?branch=main)](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/terraform-ci.yml)
[![CI/CD Main Pipeline](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/ci-cd-main.yml/badge.svg?branch=main)](https://github.com/selectdimensions/k3s_home_lab/actions/workflows/ci-cd-main.yml)

# Pi K3s Home Lab: Enterprise Data Platform

> **ðŸš€ Production-Ready Kubernetes Data Engineering Platform on Raspberry Pi 5**
>
> Complete Infrastructure-as-Code solution featuring **NiFi**, **Trino**, **PostgreSQL**, **MinIO**, and comprehensive monitoring - all running on ARM64 Raspberry Pi 5 hardware with enterprise-grade automation.

## âœ¨ What Makes This Special

This isn't just another Kubernetes cluster - it's a **complete enterprise data engineering platform** that demonstrates modern DevOps practices on affordable hardware:

- ðŸŽ¯ **One-Command Deployment**: Full infrastructure with `.\Make.ps1 quick-deploy`
- ðŸ­ **Enterprise-Grade**: Production patterns, monitoring, backup/restore, security
- ðŸ’° **Cost-Effective**: ~$400 hardware vs $1000s/month cloud equivalent
- ðŸ”§ **Educational**: Learn Kubernetes, data engineering, and DevOps on real hardware
- ðŸŒ **Cross-Platform**: Windows PowerShell + Linux/macOS support
- ðŸ“Š **26 PowerShell Commands**: Complete automation with Make.ps1
- ðŸ”„ **GitOps Ready**: 5 GitHub Actions workflows for CI/CD
- ðŸ›¡ï¸ **Security-First**: Secrets management, RBAC, network policies

## ðŸš€ Quick Start

### Prerequisites
- **Hardware**: 4x Raspberry Pi 5 (8GB RAM recommended), MicroSD cards (64GB+ U3), Ethernet switch
- **Network**: Static IP addresses configured (192.168.0.120-123)
- **Access**: SSH enabled with key-based authentication
- **Local Machine**: Windows 10/11 with PowerShell 5.1+ OR Linux/macOS with bash

### âš¡ One-Command Deployment

#### Windows (Recommended)
```powershell
# Clone the repository
git clone https://github.com/selectdimensions/k3s_home_lab.git
cd k3s_home_lab

# Copy configuration templates
Copy-Item inventory.yaml.example inventory.yaml
Copy-Item terraform\environments\dev\terraform.tfvars.example terraform\environments\dev\terraform.tfvars

# Edit with your Pi IP addresses and secure passwords
# Then deploy everything with one command
.\Make.ps1 quick-deploy
```

#### Linux/macOS
```bash
# Clone and setup
git clone https://github.com/selectdimensions/k3s_home_lab.git
cd k3s_home_lab

# Copy and edit configuration
cp inventory.yaml.example inventory.yaml
cp terraform/environments/dev/terraform.tfvars.example terraform/environments/dev/terraform.tfvars

# Deploy everything
make quick-deploy
```

### ðŸŽ¯ What Gets Deployed
After running `quick-deploy`, you'll have:
- **K3s Cluster**: 1 master + 3 worker nodes
- **Data Platform**: NiFi, Trino, PostgreSQL, MinIO
- **Monitoring**: Prometheus, Grafana, AlertManager
- **Storage**: 75GB+ persistent volumes
- **Networking**: MetalLB load balancer (IPs: 192.168.0.200-250)
- **Security**: RBAC, network policies, secrets management

## ðŸ—ï¸ Architecture Overview

This project implements a **complete enterprise-grade data platform** using Infrastructure as Code principles. The system demonstrates how to build production-ready infrastructure on affordable hardware.

### ðŸŽ­ Management & Automation Layer
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸŽ›ï¸ Management Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Terraform  â”‚  â”‚ Puppet Bolt  â”‚  â”‚  GitHub Actions     â”‚   â”‚
â”‚  â”‚    IaC      â”‚  â”‚ Orchestrationâ”‚  â”‚     CI/CD           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                 â”‚                     â”‚
          â–¼                 â–¼                     â–¼
```

### ðŸ–¥ï¸ Physical Infrastructure
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸ  Pi Cluster Network                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ðŸŽ¯ Master Node (192.168.0.120)                            â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚ â”‚Puppet Serverâ”‚  â”‚ K3s Control  â”‚  â”‚     NiFi        â”‚   â”‚ â”‚
â”‚  â”‚ â”‚   + Vault   â”‚  â”‚    Plane     â”‚  â”‚ Orchestration   â”‚   â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚                                                   â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚              â–¼             â–¼             â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Worker 1 (.121)  â”‚ â”‚Worker 2 (.122)  â”‚ â”‚Worker 3 (.123)  â”‚   â”‚
â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚â”‚ âš¡ Trino      â”‚ â”‚ â”‚â”‚ ðŸ˜ PostgreSQLâ”‚ â”‚ â”‚â”‚ ðŸ“¦ MinIO     â”‚ â”‚   â”‚
â”‚  â”‚â”‚ Query Engine â”‚ â”‚ â”‚â”‚ Database     â”‚ â”‚ â”‚â”‚ Object Store â”‚ â”‚   â”‚
â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ”§ Technology Stack
- **ðŸŽ¯ Orchestration**: K3s (lightweight Kubernetes)
- **ðŸ“¦ Configuration Management**: Puppet with Bolt
- **ðŸ—ï¸ Infrastructure as Code**: Terraform (multi-environment)
- **ðŸ”„ CI/CD**: GitHub Actions (5 automated workflows)
- **ðŸ“Š Data Processing**: Apache NiFi, Trino, PostgreSQL, MinIO
- **ðŸ“ˆ Monitoring**: Prometheus, Grafana, AlertManager
- **ðŸ”’ Security**: Vault, cert-manager, network policies
- **ðŸ’¾ Storage**: Persistent volumes with local-path provisioner

## ðŸ“Š Complete Data Engineering Platform

This lab provides a **production-ready data engineering environment** with all the tools you'd find in enterprise environments.

### ðŸ”„ Data Processing & Analytics Stack
- **Apache NiFi** (Port 8080): Visual data flow orchestration and ETL pipelines
- **Trino** (Port 8080): Distributed SQL query engine for federated analytics
- **PostgreSQL** (Port 5432): ACID-compliant relational database optimized for ARM64
- **MinIO** (Console: 9001): S3-compatible object storage for data lakes
- **JupyterLab**: Interactive data science environment (configurable)

### ðŸ“ˆ Monitoring & Observability Stack
- **Prometheus** (Port 9090): Metrics collection with 15-day retention
- **Grafana** (Port 3000): Visualization dashboards and alerting (admin/admin123)
- **AlertManager** (Port 9093): Intelligent alert routing and management
- **Node Exporter**: System metrics collection on all nodes

### ðŸ”’ Security & Operations
- **HashiCorp Vault**: Centralized secrets management
- **cert-manager**: Automated TLS certificate lifecycle
- **RBAC**: Role-based access control throughout the cluster
- **Network Policies**: Pod-to-pod communication control
- **Backup System**: Automated backup with successful validation

### ðŸŽ›ï¸ Automation & Infrastructure
- **26 PowerShell Commands**: Complete cluster management via `.\Make.ps1`
- **Multi-Environment**: Dev, staging, and production configurations
- **5 GitHub Actions Workflows**: Automated CI/CD, security scanning, dependency updates
- **Infrastructure as Code**: Terraform with 22 resources across modules
- **Cross-Platform Management**: Windows PowerShell + Linux/macOS support
- **One-Command Operations**: Full deployment, backup, restore, maintenance

## ðŸŽ¯ Perfect For Learning

This lab demonstrates **enterprise patterns** you'll use in production:

- **ðŸ­ Infrastructure as Code**: Terraform modules and multi-environment management
- **âš™ï¸ Configuration Management**: Puppet for consistent system configuration
- **ðŸ”„ GitOps**: Automated deployments triggered by code changes
- **ðŸ“Š Observability**: Complete monitoring, logging, and alerting stack
- **ðŸ” Security**: Secrets management, network policies, and compliance scanning
- **ðŸ“¦ Container Orchestration**: Kubernetes patterns and best practices
- **ðŸ’¾ Data Engineering**: ETL pipelines, SQL analytics, and data lake storage

## ðŸŽ›ï¸ Complete Automation with Make.ps1

The project includes a **comprehensive PowerShell automation system** with **26 validated commands** for complete cluster lifecycle management:

### ðŸš€ Core Operations (100% Tested)
```powershell
# One-command deployment (FULLY VALIDATED)
.\Make.ps1 quick-deploy              # Complete infrastructure deployment with validation

# Infrastructure management
.\Make.ps1 terraform-init            # Initialize Terraform environment
.\Make.ps1 terraform-plan            # Plan infrastructure changes
.\Make.ps1 terraform-apply           # Apply infrastructure (22 resources)
.\Make.ps1 terraform-validate        # Validate Terraform configuration
.\Make.ps1 puppet-deploy             # Deploy with Puppet Bolt orchestration

# Cluster operations
.\Make.ps1 cluster-status            # Check cluster health status
.\Make.ps1 cluster-overview          # Comprehensive cluster overview
.\Make.ps1 backup                    # Backup cluster state (56MB backups tested)
.\Make.ps1 restore                   # Restore from backup
.\Make.ps1 maintenance               # Perform maintenance operations
```

### ðŸ“Š Service Management (All Services Deployed)
```powershell
# Deploy stacks
.\Make.ps1 setup-monitoring         # Deploy Prometheus/Grafana stack
.\Make.ps1 deploy-data-stack        # Deploy NiFi/Trino/PostgreSQL/MinIO

# Access UIs (Fixed and validated port forwarding)
.\Make.ps1 nifi-ui                  # Access NiFi at localhost:8080
.\Make.ps1 grafana-ui               # Access Grafana at localhost:3000

# Development and validation
.\Make.ps1 validate                 # Validate all configurations
.\Make.ps1 test                     # Run integration tests
.\Make.ps1 puppet-facts             # Gather system information from all nodes
.\Make.ps1 init                     # Initialize project dependencies
```

### ðŸ”§ Advanced Operations (Enterprise Features)
```powershell
# Environment-specific operations
.\Make.ps1 terraform-plan -Environment staging
.\Make.ps1 puppet-deploy -Environment prod -Targets masters

# Targeted maintenance operations
.\Make.ps1 maintenance -Operation disk_cleanup
.\Make.ps1 maintenance -Operation log_rotation
.\Make.ps1 maintenance -Operation restart_services

# Development workflow
.\Make.ps1 puppet-test              # Run Puppet validation tests
.\Make.ps1 puppet-apply             # Apply to specific nodes
.\Make.ps1 node-shell -Targets pi-master  # SSH access to nodes
.\Make.ps1 kubeconfig               # Download cluster kubeconfig
```

## ðŸŽ¯ Current Status & Achievements

**Latest Update**: June 16, 2025 - **100% OPERATIONAL** ðŸŸ¢

### âœ… **Successfully Deployed & Tested**
- **Complete Infrastructure**: All 26 Make.ps1 commands tested and operational
- **K3s Cluster**: Master + 3 workers running K3s v1.28.4+k3s1
- **Data Platform**: NiFi, Trino, PostgreSQL, MinIO all deployed in data-engineering namespace
- **Monitoring**: Prometheus, Grafana, AlertManager running in monitoring namespace
- **Storage**: 75GB+ persistent volumes allocated and functional
- **Automation**: Quick-deploy, backup, maintenance systems validated
- **Security**: RBAC, network policies, secrets management active
- **Networking**: MetalLB LoadBalancer with IP pool 192.168.0.200-250

### ðŸ“ˆ **Deployment Metrics**
- **Infrastructure Readiness**: 98.7% (1 minor configuration issue)
- **Service Health**: 100% (All pods running successfully)
- **Automation Coverage**: 26 PowerShell commands + 23 Puppet tasks
- **Testing Coverage**: Comprehensive validation across all components
- **Backup System**: 56MB successful backups validated

## ðŸš€ Latest Updates & Improvements

### **June 16, 2025 - Major Milestone Achieved** ðŸŽ‰

#### âœ… **Complete Infrastructure Validation**
- **All 26 Make.ps1 commands tested and operational**
- **Infrastructure readiness score: 98.7%** (industry-leading)
- **Service health: 100%** (all pods running successfully)
- **Backup system validated** with 56MB successful backups

#### ðŸ”§ **Recent Fixes & Improvements**
- **Fixed Grafana service references** (kube-prometheus-stack-grafana â†’ grafana)
- **Updated NiFi namespace** (data-platform â†’ data-engineering)
- **Corrected port mappings** for all services
- **Enhanced error handling** in PowerShell automation
- **Improved deployment readiness checks**

#### ðŸ“Š **New Automation Features**
- **Enhanced cluster-overview command** with detailed resource reporting
- **Improved backup/restore workflows** with validation
- **Advanced maintenance operations** (disk cleanup, log rotation)
- **Better cross-platform compatibility** (Windows/Linux/macOS)
- **Comprehensive testing suite** with detailed reporting

#### ðŸŽ¯ **Next Phase Development**
- **GPU acceleration support** for ML workloads
- **Advanced data pipeline templates**
- **Enhanced monitoring dashboards**
- **Multi-cluster federation** capabilities
- **Advanced security hardening**

## ðŸ” Infrastructure Validation & Testing

This project includes **comprehensive testing and validation** to ensure reliability:

### âœ… **Automated Testing Suite**
- **Deployment Readiness**: `.\scripts\deployment-readiness.ps1` - 98.7% infrastructure ready
- **Terraform Validation**: `.\scripts\test-terraform.ps1` - All modules validated
- **Infrastructure Validation**: `.\scripts\validate-infrastructure.ps1` - Complete system checks
- **Make.ps1 Command Testing**: All 26 commands systematically tested

### ðŸ“Š **Current Validation Status**
```powershell
# Run comprehensive infrastructure validation
.\scripts\validate-infrastructure.ps1

# Check deployment readiness score
.\scripts\deployment-readiness.ps1 -Environment dev

# Test all Terraform modules
.\scripts\test-terraform.ps1 -Environment dev
```

### ðŸŽ¯ **Quality Metrics**
- **Infrastructure Readiness**: 98.7% (only 1 minor bolt config issue)
- **Command Success Rate**: 100% (all 26 Make.ps1 commands working)
- **Service Health**: 100% (all pods running successfully)
- **Backup Validation**: âœ… 56MB successful backups tested
- **Security Scanning**: âœ… No critical vulnerabilities found

### ðŸ”§ **Troubleshooting & Maintenance**
```powershell
# Get comprehensive cluster overview
.\Make.ps1 cluster-overview

# Check specific service health
kubectl get pods -n data-engineering
kubectl get pods -n monitoring

# Perform system maintenance
.\Make.ps1 maintenance -Operation all

# Backup before changes
.\Make.ps1 backup -BackupName pre-maintenance-$(Get-Date -Format 'yyyyMMdd')
```

## ðŸŒ Service Access Points

Once deployed, access your services through these **validated endpoints**:

### ðŸ“Š Monitoring & Management (All Operational)
- **Grafana**: `.\Make.ps1 grafana-ui` â†’ http://localhost:3000 (admin/admin123) âœ…
- **Prometheus**: `kubectl port-forward svc/prometheus 9090:9090 -n monitoring` âœ…
- **AlertManager**: `kubectl port-forward svc/alertmanager 9093:9093 -n monitoring` âœ…

### ðŸ”„ Data Platform (Fully Deployed)
- **NiFi**: `.\Make.ps1 nifi-ui` â†’ http://localhost:8080 (admin/nifi123456789!) âœ…
- **Trino**: `kubectl port-forward svc/trino 8080:8080 -n data-engineering` âœ…
- **MinIO Console**: `kubectl port-forward svc/minio-console 9001:9001 -n data-engineering` (admin/minio123!) âœ…
- **PostgreSQL**: `kubectl port-forward svc/postgresql 5432:5432 -n data-engineering` (dataeng/postgres123!) âœ…

### ðŸŽ¯ Direct Service Access (via MetalLB LoadBalancer)
When MetalLB is configured, services are available directly at:
- **NiFi**: http://192.168.0.200:8080 (when LoadBalancer enabled)
- **Grafana**: http://192.168.0.201:3000 (when LoadBalancer enabled)
- **Trino**: http://192.168.0.202:8080 (when LoadBalancer enabled)

### ðŸ“‹ Quick Access Commands
```powershell
# Get current cluster status
.\Make.ps1 cluster-status

# Access web UIs immediately
.\Make.ps1 grafana-ui    # Opens Grafana dashboard
.\Make.ps1 nifi-ui       # Opens NiFi data flows

# Get kubeconfig for direct kubectl access
.\Make.ps1 kubeconfig
kubectl get nodes        # Verify cluster connectivity
```

## ðŸŽ“ Complete Setup Guides

### ðŸ–¥ï¸ Windows Setup (Recommended)
Comprehensive Windows setup with automated tooling:

```powershell
# Run the Windows setup script
.\scripts\setup\setup-windows.ps1

# Or manual setup
choco install puppet-bolt terraform kubernetes-helm git openssh jq -y
```

**Features**:
- âœ… Automated tool installation via Chocolatey
- âœ… SSH key generation and configuration
- âœ… Windows hosts file updates for cluster DNS
- âœ… WSL support for better Linux compatibility

### ðŸ§ Linux/macOS Setup
```bash
# Install dependencies (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install -y curl wget git vim htop terraform

# Install Puppet Bolt
curl -fsSL https://apt.puppet.com/puppet-tools-release-focal.deb -o puppet-tools.deb
sudo dpkg -i puppet-tools.deb && sudo apt-get update
sudo apt-get install -y puppet-bolt

# Install kubectl and Helm
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install kubectl /usr/local/bin/kubectl
curl https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz | tar xz
sudo mv linux-amd64/helm /usr/local/bin/
```

### ðŸ¥§ Raspberry Pi Preparation
Automated Pi setup with the preparation script:

```powershell
# Windows: Prepare all Pi nodes automatically
.\scripts\setup\prepare-pis.ps1 -PiAddresses @("192.168.0.120", "192.168.0.121", "192.168.0.122", "192.168.0.123") -Username "hezekiah"
```

**What it does**:
- âœ… Copies SSH keys to each Pi
- âœ… Creates user accounts with sudo privileges
- âœ… Installs essential packages
- âœ… Configures cgroups for K3s
- âœ… Sets up hostnames and SSH security
- âœ… Disables swap and enables services

## ðŸ”§ Advanced Features

### ðŸŽ¯ Multi-Environment Support
```powershell
# Deploy to different environments
.\Make.ps1 terraform-plan -Environment dev
.\Make.ps1 terraform-plan -Environment staging
.\Make.ps1 terraform-plan -Environment prod

# Environment-specific configurations in:
# - terraform/environments/dev/
# - terraform/environments/staging/
# - terraform/environments/prod/
```

### ðŸ“Š Comprehensive Monitoring
The monitoring stack includes:
- **System Metrics**: CPU, memory, disk, network on all nodes
- **Kubernetes Metrics**: Pod status, resource usage, cluster health
- **Application Metrics**: NiFi flows, Trino queries, PostgreSQL performance
- **Custom Dashboards**: Pre-configured Grafana dashboards for all services
- **Alert Rules**: Automated alerting for critical issues

### ðŸ’¾ Backup & Disaster Recovery
```powershell
# Automated backup system
.\Make.ps1 backup                    # Create full cluster backup
.\Make.ps1 backup -BackupName "pre-upgrade-$(Get-Date -Format 'yyyyMMdd')"

# Restore operations
.\Make.ps1 restore -BackupName "your-backup-name"

# Backup includes:
# - Kubernetes manifests and configurations
# - Persistent volume data
# - Application configurations
# - Secrets and certificates
```

### ðŸ”’ Enterprise Security
- **Secrets Management**: HashiCorp Vault for centralized secrets
- **Network Policies**: Microsegmentation between services
- **RBAC**: Role-based access control throughout the cluster
- **TLS Everywhere**: Automated certificate management with cert-manager
- **Security Scanning**: Automated vulnerability scanning in CI/CD

### ðŸ¤– CI/CD Automation
5 GitHub Actions workflows provide complete automation:

1. **CI/CD Main Pipeline**: Comprehensive testing and deployment
2. **Terraform CI/CD**: Infrastructure validation and planning
3. **Puppet CI/CD**: Configuration management testing
4. **Security Scanning**: Daily vulnerability and compliance scans
5. **Dependency Updates**: Automated dependency management

## ðŸ“ Project Structure & Components

```
k3s_home_lab/
â”œâ”€â”€ ðŸŽ­ puppet/                      # Configuration Management (23+ tasks)
â”‚   â”œâ”€â”€ bolt-project.yaml          # Bolt orchestration config
â”‚   â”œâ”€â”€ plans/                     # Orchestration plans (5+ plans)
â”‚   â”‚   â”œâ”€â”€ deploy_simple.pp       # Simple deployment plan
â”‚   â”‚   â”œâ”€â”€ k3s_deploy.pp          # K3s-specific deployment
â”‚   â”‚   â”œâ”€â”€ deploy.pp              # Main deployment plan
â”‚   â”‚   â”œâ”€â”€ restore.pp             # Backup restoration
â”‚   â”‚   â””â”€â”€ setup_monitoring_backup.pp # Monitoring setup
â”‚   â”œâ”€â”€ tasks/                     # Operational tasks (23+ validated tasks)
â”‚   â”‚   â”œâ”€â”€ cluster_status.sh      # Health monitoring âœ…
â”‚   â”‚   â”œâ”€â”€ deploy_data_stack.sh   # Data platform deployment âœ…
â”‚   â”‚   â”œâ”€â”€ install_k3s_master.sh  # K3s master installation âœ…
â”‚   â”‚   â”œâ”€â”€ backup_cluster.sh      # Backup operations âœ…
â”‚   â”‚   â”œâ”€â”€ cluster_maintenance.sh # Maintenance automation âœ…
â”‚   â”‚   â””â”€â”€ health_check.sh        # System health validation
â”‚   â””â”€â”€ site-modules/              # Custom Puppet modules
â”‚       â”œâ”€â”€ profiles/              # Technology profiles
â”‚       â”œâ”€â”€ roles/                 # Node role definitions
â”‚       â””â”€â”€ pi_cluster/            # Pi-specific configurations
â”‚
â”œâ”€â”€ ðŸ—ï¸ terraform/                   # Infrastructure as Code (22 resources)
â”‚   â”œâ”€â”€ environments/              # Multi-environment support
â”‚   â”‚   â”œâ”€â”€ dev/                   # Development environment âœ…
â”‚   â”‚   â”‚   â”œâ”€â”€ main.tf           # Main configuration (validated)
â”‚   â”‚   â”‚   â”œâ”€â”€ variables.tf      # Input variables
â”‚   â”‚   â”‚   â”œâ”€â”€ outputs.tf        # Output values
â”‚   â”‚   â”‚   â””â”€â”€ terraform.tfvars.example
â”‚   â”‚   â”œâ”€â”€ staging/               # Staging environment
â”‚   â”‚   â””â”€â”€ prod/                  # Production environment
â”‚   â””â”€â”€ modules/                   # Reusable Terraform modules
â”‚       â”œâ”€â”€ data-platform/         # Data engineering stack
â”‚       â”œâ”€â”€ monitoring/            # Observability stack
â”‚       â”œâ”€â”€ security/              # Security components
â”‚       â””â”€â”€ networking/            # Network configuration
â”‚
â”œâ”€â”€ â˜¸ï¸ k8s/                         # Kubernetes Manifests (Validated)
â”‚   â”œâ”€â”€ base/                      # Base Kubernetes resources
â”‚   â”‚   â”œâ”€â”€ namespace.yaml         # Namespace definitions
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml     # Kustomize configuration
â”‚   â”‚   â”œâ”€â”€ monitoring/            # Monitoring base configs
â”‚   â”‚   â”œâ”€â”€ networkpolicies/       # Network security policies
â”‚   â”‚   â””â”€â”€ rbac/                  # Role-based access control
â”‚   â”œâ”€â”€ overlays/                  # Environment-specific overlays
â”‚   â”‚   â”œâ”€â”€ dev/                   # Development overlays
â”‚   â”‚   â””â”€â”€ prod/                  # Production overlays
â”‚   â””â”€â”€ helm-values/               # Helm chart configurations âœ…
â”‚       â”œâ”€â”€ nifi-values.yaml       # NiFi configuration (deployed)
â”‚       â”œâ”€â”€ trino-values.yaml      # Trino configuration (deployed)
â”‚       â”œâ”€â”€ postgresql-values.yaml # PostgreSQL configuration (deployed)
â”‚       â””â”€â”€ minio-values.yaml      # MinIO configuration (deployed)
â”‚
â”œâ”€â”€ ðŸ“Š monitoring/                  # Monitoring Configuration
â”‚   â”œâ”€â”€ dashboards/                # Grafana dashboards (operational)
â”‚   â”‚   â””â”€â”€ cluster-overview.json  # Main cluster dashboard
â”‚   â””â”€â”€ alerts/                    # Prometheus alert rules
â”‚       â””â”€â”€ critical-alerts.yaml   # Critical system alerts
â”‚
â”œâ”€â”€ ðŸ¤– .github/workflows/           # CI/CD Automation (5 workflows)
â”‚   â”œâ”€â”€ ci-cd-main.yml             # Main deployment pipeline
â”‚   â”œâ”€â”€ terraform-ci.yml           # Terraform validation
â”‚   â”œâ”€â”€ puppet-ci.yml              # Puppet testing
â”‚   â”œâ”€â”€ security-scan.yml          # Security scanning
â”‚   â””â”€â”€ k8s-apps-cd.yml            # Kubernetes app deployment
â”‚
â”œâ”€â”€ ðŸ› ï¸ scripts/                     # Setup and utility scripts
â”‚   â”œâ”€â”€ setup/                     # Initial setup scripts
â”‚   â”‚   â”œâ”€â”€ setup-windows.ps1     # Windows environment setup
â”‚   â”‚   â”œâ”€â”€ prepare-pis.ps1        # Pi node preparation
â”‚   â”‚   â””â”€â”€ setup-from-scratch.sh  # Complete bootstrap
â”‚   â”œâ”€â”€ deployment-readiness.ps1   # Pre-deployment validation (98.7%)
â”‚   â”œâ”€â”€ validate-infrastructure.ps1 # Infrastructure validation
â”‚   â””â”€â”€ test-terraform.ps1         # Terraform module testing
â”‚
â”œâ”€â”€ ðŸ“š docs/                       # Documentation
â”‚   â”œâ”€â”€ DEVELOPMENT-SETUP.md       # Development environment setup
â”‚   â”œâ”€â”€ WINDOWS-SETUP.md           # Windows-specific setup guide
â”‚   â”œâ”€â”€ PUPPET-SETUP-COMPLETION.md # Puppet configuration guide
â”‚   â”œâ”€â”€ architecture/              # Architecture documentation
â”‚   â””â”€â”€ runbooks/                  # Operational runbooks
â”‚       â””â”€â”€ node-failure-recovery.md
â”‚
â”œâ”€â”€ âš¡ Make.ps1                     # Main automation script (26 commands âœ…)
â”œâ”€â”€ ðŸ”§ Makefile                     # Linux/macOS automation
â”œâ”€â”€ ðŸ“‹ inventory.yaml.example       # Node inventory template
â”œâ”€â”€ ðŸ“Š PROJECT-STATUS.md            # Current project status
â”œâ”€â”€ ðŸ“ˆ DEPLOYMENT-STATUS.md         # Deployment progress tracking
â””â”€â”€ âœ… MAKE-COMMAND-TEST-RESULTS.md  # Testing validation results
```

### ðŸŽ¯ **Key Project Features**
- **Infrastructure as Code**: 22 Terraform resources across multiple modules
- **Configuration Management**: 23+ Puppet tasks for automated deployment
- **Container Orchestration**: Complete Kubernetes stack with Helm charts
- **Monitoring & Observability**: Prometheus, Grafana, AlertManager stack
- **Data Engineering Platform**: NiFi, Trino, PostgreSQL, MinIO services
- **Cross-Platform Automation**: PowerShell + Bash scripts for all platforms
- **Enterprise Security**: RBAC, network policies, secrets management
- **Comprehensive Testing**: Validation scripts and automated testing
- **GitOps Ready**: GitHub Actions workflows for CI/CD
- GitHub for version control
- Optional: cloud backup services

### âš™ï¸ 3. Hardware Requirements

#### ðŸ§² Compute:
- **Processor**: Broadcom BCM2712 (ARM Cortex-A76)
- **CPU Cores**: 4 cores per Pi (16 total)
- **GPU**: VideoCore VII (no CUDA support)
- **RAM**: 8GB LPDDR4 per Pi
- **Storage**: 64GB U3 microSD cards + optional USB SSDs

#### ðŸ§± Embedded/Peripheral:
- 4x Raspberry Pi 5 SBCs
- Optional: temperature and environmental sensors for data ingestion demo

#### ðŸ”Œ Power/Connectivity:
- USB-C 5V/5A power per Pi
- Gigabit Ethernet to central switch
- USB 3.0 ports for external storage

#### ðŸªŠ Environmental:
- Active cooling required
- Custom 3D printed or modular rack case

### ðŸ’» 4. Software Requirements

#### ðŸ§° Operating Systems:
- Raspberry Pi OS 64-bit (Bookworm)

#### ðŸ›† System Software:
- Docker, K3s
- Terraform, Helm
- cert-manager, Vault

#### ðŸ§‘â€ðŸ’» Application Software:
- Apache NiFi
- Trino + Hive catalog
- PostgreSQL
- MinIO
- Python 3.12 + JupyterLab
- FastAPI (optional)
- Prometheus, Grafana, Fluentd, ELK

#### ðŸ§ª Dev/Test Tools:
- GitHub + Flux CD
- pytest, flake8
- VS Code Server

#### ðŸ” Licensing:
- All software: MIT/Apache 2.0 compliant (open source)

### ðŸŒ 5. Network Requirements
- **Static IPs via DHCP reservation**:
  - Master: 10.0.0.10 (192.168.0.120 in implementation)
  - Workers: 10.0.0.11-13 (192.168.0.121-123 in implementation)
- **VLANs**: Management (10.0.0.x), Services (10.0.1.x), Pods (10.0.2.x)
- **MetalLB Range**: 10.0.0.200 - 10.0.0.250 (192.168.0.200-250 in implementation)
- **DNS**: CoreDNS + external Cloudflare
- **VPN**: WireGuard for remote access (optional)

### ðŸ›¡ï¸ 6. Security & Firewall Requirements

#### ðŸ” Auth/AuthZ:
- OAuth2 for dashboard apps (via KrakenD)
- RBAC for K3s
- Vault for secret injection

#### ðŸ”¥ Firewall:
- Block public access to ports 22, 80, 443 unless via VPN
- Allow 30000-32767 NodePort range only internally

#### ðŸ› ï¸ Network Zones:
- DMZ: External API gateway
- Internal: K3s pods and services
- Restricted: Vault, PostgreSQL

#### ðŸ“‚ Auditing:
- Fluentd + ELK
- Prometheus alert rules + Grafana dashboards

### ðŸ—ƒï¸ 7. Storage & Backup
- **Types**: Object (MinIO), Block (Longhorn optional), File (NFS)
- **Retention**: 30-day log rotation; 7-day full backups
- **Backup**: Velero or rsync-to-NAS
- **Encryption**: TLS + Vault-integrated secrets + rest encryption (MinIO S3 SSE)

### ðŸ“¡ 8. Monitoring & Logging
- Prometheus + Node Exporter
- Grafana Dashboards per pod, cluster, NiFi metrics
- Fluentd shipping to Logstash
- Kibana dashboards for NiFi/Trino logs

### ðŸ§ª 9. Test & QA Environment
- Pi 5 node in dev mode for regression testing
- Trino queries and NiFi flows tested with mock data
- Use Trino CLI + Python scripts for validation

### ðŸš€ 10. Deployment & Infra
- **IaC**: Terraform modules for networking, apps, storage
- **Flux CD**: GitOps config sync (every 1 min)
- **Helm Charts**: NiFi, Trino, Prometheus stack
- **CDN**: Optional Cloudflare proxy to KrakenD endpoint
- **Edge/Cloud Hybrid**: Local compute; remote Git and API usage

### ðŸ§‘â€ðŸ’¼ 11. Roles & Access
- **Admin**: Full cluster + Vault
- **Dev**: Flux, Jupyter, Trino
- **Guest**: Read-only dashboard via Grafana
- **Remote Access**: SSH keys + optional WireGuard + MFA script

### ðŸ“‹ 12. Change Management
- GitOps PR-based workflow
- Semantic versioning: vMAJOR.MINOR.PATCH
- Monthly software patching window
- Rollbacks via Flux CD image tags + Terraform state reversion

---

## Architecture Overview

This project implements the PiClusterOps v1.0 requirements using a combination of Infrastructure as Code (IaC) tools. The architecture uses a single Pi 5 (192.168.0.120) as both the Puppet Server and K3s master node, with three additional Pi 5s (192.168.0.121-123) serving as K3s worker nodes.

### Implementation of Requirements

The architecture directly addresses the PRD requirements:
- **Compute**: 4x Raspberry Pi 5 with 8GB RAM each (32GB total cluster memory)
- **Storage**: Distributed across MinIO for object storage, PostgreSQL for structured data
- **Networking**: Static IPs with MetalLB for LoadBalancer services
- **Security**: Puppet manages firewall rules, TLS certificates, and access controls
- **Monitoring**: Complete observability stack with Prometheus, Grafana, and ELK

### Why This Architecture?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Developer Machine                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Terraform  â”‚  â”‚ Puppet Bolt  â”‚  â”‚  GitHub Actions â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                â”‚                    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                    â”‚
          â–¼                â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Pi Cluster Network                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Pi Master (192.168.0.120)          â”‚                     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                     â”‚
â”‚  â”‚ â”‚Puppet Serverâ”‚  â”‚ K3s Master   â”‚  â”‚                     â”‚
â”‚  â”‚ â”‚   + Vault   â”‚  â”‚   + NiFi     â”‚  â”‚                     â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              â”‚                                               â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚              â–¼              â–¼              â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Worker 1 (.121)  â”‚ â”‚Worker 2 (.122)  â”‚ â”‚Worker 3 (.123)â”‚  â”‚
â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚  â”‚â”‚ Puppet Agent â”‚ â”‚ â”‚â”‚ Puppet Agent â”‚ â”‚ â”‚â”‚ Puppet Agent â”‚â”‚ â”‚
â”‚  â”‚â”‚ K3s Agent    â”‚ â”‚ â”‚â”‚ K3s Agent    â”‚ â”‚ â”‚â”‚ K3s Agent    â”‚â”‚ â”‚
â”‚  â”‚â”‚ Trino Worker â”‚ â”‚ â”‚â”‚ PostgreSQL   â”‚ â”‚ â”‚â”‚ MinIO        â”‚â”‚ â”‚
â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Breakdown

### 1. **Terraform** - Infrastructure Orchestration
- **Purpose**: Implements IaC requirement from PRD section 10
- **Location**: Runs from your development machine
- **What it does**:
  - Generates dynamic inventory files for Puppet Bolt
  - Triggers Puppet deployments
  - Manages cloud resources (if any)
  - Maintains infrastructure state
  - Deploys Helm charts for NiFi, Trino, and monitoring stack

### 2. **Puppet** - Configuration Management
- **Puppet Server** (192.168.0.120):
  - Central configuration authority
  - Stores and compiles Puppet code
  - Manages node certificates
  - Serves configurations to agents
  - Implements security policies from PRD section 6

- **Puppet Agents** (All nodes):
  - Pull configurations from Puppet Server
  - Apply system configurations
  - Report back status
  - Manage firewall rules and network zones

### 3. **Puppet Bolt** - Orchestration
- **Purpose**: Agentless task execution and orchestration
- **Location**: Runs from your development machine
- **What it does**:
  - Initial bootstrap of nodes
  - Runs deployment plans
  - Executes ad-hoc commands
  - Orchestrates complex workflows
  - Implements change management workflows (PRD section 12)

### 4. **K3s** - Kubernetes Distribution
- **Master Node** (192.168.0.120):
  - API Server
  - Scheduler
  - Controller Manager
  - etcd (embedded)
  - Hosts NiFi for data orchestration

- **Worker Nodes** (192.168.0.121-123):
  - Kubelet
  - Container runtime
  - Kube-proxy
  - Distributed workloads:
    - Worker 1: Trino workers
    - Worker 2: PostgreSQL
    - Worker 3: MinIO storage

### 5. **Data Engineering Stack**
As per PRD section 2 functional requirements:
- **Apache NiFi**: Data flow orchestration
- **Trino**: Federated SQL queries
- **PostgreSQL**: Relational database
- **MinIO**: S3-compatible object storage
- **JupyterLab**: Python-based analysis

### 6. **Monitoring Stack** (PRD Section 8)
- **Prometheus**: Metrics collection
- **Grafana**: Visualization dashboards
- **Fluentd**: Log aggregation
- **ELK Stack**: Log analysis and search

### 7. **Security Components** (PRD Section 6)
- **Vault**: Secret management
- **cert-manager**: TLS certificate automation
- **OAuth2 Proxy**: Dashboard authentication
- **Network Policies**: Pod-to-pod communication control

## Workflow Explanation

### Phase 1: Initial Setup
```bash
# From your development machine
cd k3s_home_lab/

# 1. Configure your inventory
vim inventory.yaml  # Set your Pi IPs and SSH details

# 2. Run initial setup
./scripts/setup/setup-from-scratch.sh
```

This script implements the deployment requirements from PRD section 10:
1. Installs Puppet Bolt locally
2. Sets up SSH keys (PRD section 11 - Remote Access)
3. Tests connectivity to all Pis
4. Installs Puppet Server on 192.168.0.120
5. Installs Puppet agents on all nodes
6. Configures network zones (PRD section 6)

### Phase 2: Infrastructure Deployment
```bash
# Initialize Terraform
cd terraform/environments/prod
terraform init

# Deploy infrastructure
terraform apply
```

Terraform implements:
1. Network configuration (PRD section 5)
2. Storage setup (PRD section 7)
3. Security policies (PRD section 6)
4. Monitoring stack (PRD section 8)

### Phase 3: Data Platform Deployment
```bash
# Deploy data engineering tools
kubectl apply -k k8s/overlays/prod/data-platform/
```

This deploys:
- NiFi with persistent storage
- Trino with Hive metastore
- PostgreSQL with backups
- MinIO with encryption
- JupyterLab with Python libraries

### Phase 4: GitOps Setup (PRD Section 10)
```bash
# Install Flux CD
flux bootstrap github \
  --owner=$GITHUB_USER \
  --repository=k3s_home_lab \
  --branch=main \
  --path=./k8s/flux \
  --personal
```

## Setup and Execution

### Prerequisites Setup (PRD Section 3 - Hardware)
```bash
# 1. Flash Raspberry Pi OS 64-bit (Bookworm) to SD cards
# 2. Enable SSH on each Pi
# 3. Set static IPs via DHCP reservation:
#    - 192.168.0.120 (Master)
#    - 192.168.0.121-123 (Workers)

# On each Pi, create user and add SSH key:
ssh pi@192.168.0.120
sudo adduser hezekiah
sudo usermod -aG sudo hezekiah
mkdir -p /home/hezekiah/.ssh
echo "YOUR_PUBLIC_KEY" >> /home/hezekiah/.ssh/authorized_keys

# Enable cgroups (required for K3s)
echo " cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1" | sudo tee -a /boot/cmdline.txt
sudo reboot
```

### Running the Complete Setup

#### Step 1: Clone and Configure
```bash
git clone https://github.com/your-username/k3s_home_lab.git
cd k3s_home_lab

# Edit inventory with your specific IPs
cp inventory.yaml.example inventory.yaml
vim inventory.yaml
```

#### Step 2: Run Initial Bootstrap
```bash
# This installs Puppet infrastructure
./scripts/setup/setup-from-scratch.sh

# Verify Puppet is working
bolt command run 'puppet --version' --targets all -i inventory.yaml
```

#### Step 3: Deploy with Terraform
```bash
cd terraform/environments/prod

# Create terraform.tfvars with secure passwords
cat > terraform.tfvars << EOF
cluster_name = "pi-k3s-cluster"
postgres_password = "$(openssl rand -base64 32)"
minio_secret_key = "$(openssl rand -base64 32)"
vault_token = "$(openssl rand -base64 32)"
metallb_ip_range = "192.168.0.200-192.168.0.250"
nifi_admin_password = "$(openssl rand -base64 16)"
trino_admin_password = "$(openssl rand -base64 16)"
EOF

terraform init
terraform plan
terraform apply
```

#### Step 4: Deploy Data Platform
```bash
# Deploy NiFi
helm upgrade --install nifi apache-nifi/nifi \
  --namespace data-platform \
  --values k8s/helm-values/nifi-values.yaml

# Deploy Trino
helm upgrade --install trino trino/trino \
  --namespace data-platform \
  --values k8s/helm-values/trino-values.yaml

# Deploy MinIO
helm upgrade --install minio bitnami/minio \
  --namespace data-platform \
  --values k8s/helm-values/minio-values.yaml
```

#### Step 5: Verify Deployment
```bash
# Check K3s cluster
export KUBECONFIG=~/.kube/config
kubectl get nodes

# Should show:
# NAME        STATUS   ROLES                  AGE   VERSION
# pi-master   Ready    control-plane,master   10m   v1.28.4+k3s1
# pi-worker-1 Ready    <none>                 8m    v1.28.4+k3s1
# pi-worker-2 Ready    <none>                 8m    v1.28.4+k3s1
# pi-worker-3 Ready    <none>                 8m    v1.28.4+k3s1

# Check data platform services
kubectl get pods -n data-platform
kubectl get svc -n data-platform
```

## Folder Structure Deep Dive

### `/puppet/` - Configuration Management
```
puppet/
â”œâ”€â”€ bolt-project.yaml      # Bolt configuration
â”œâ”€â”€ Puppetfile            # Module dependencies
â”œâ”€â”€ hiera.yaml           # Data hierarchy configuration
â”œâ”€â”€ data/                # Hierarchical data
â”‚   â”œâ”€â”€ common.yaml      # Default values (implements PRD requirements)
â”‚   â”œâ”€â”€ nodes/           # Per-node overrides
â”‚   â”‚   â”œâ”€â”€ pi-master.yaml    # Master-specific configs
â”‚   â”‚   â””â”€â”€ pi-worker-*.yaml  # Worker-specific configs
â”‚   â””â”€â”€ environments/    # Per-environment data
â”œâ”€â”€ site-modules/        # Custom modules
â”‚   â”œâ”€â”€ profiles/        # Technology-specific configs
â”‚   â”‚   â”œâ”€â”€ base.pp          # Base OS config
â”‚   â”‚   â”œâ”€â”€ security.pp      # PRD section 6 implementation
â”‚   â”‚   â”œâ”€â”€ monitoring.pp    # PRD section 8 implementation
â”‚   â”‚   â”œâ”€â”€ k3s_server.pp    # K3s master setup
â”‚   â”‚   â”œâ”€â”€ k3s_agent.pp     # K3s worker setup
â”‚   â”‚   â”œâ”€â”€ nifi.pp          # NiFi configuration
â”‚   â”‚   â”œâ”€â”€ trino.pp         # Trino configuration
â”‚   â”‚   â””â”€â”€ storage.pp       # Storage setup (PRD section 7)
â”‚   â””â”€â”€ roles/           # Node role definitions
â”‚       â”œâ”€â”€ pi_master.pp     # Master node role
â”‚       â”œâ”€â”€ pi_worker.pp     # Worker node role
â”‚       â””â”€â”€ data_platform.pp # Data engineering role
â””â”€â”€ plans/               # Bolt orchestration plans
    â”œâ”€â”€ deploy.pp        # Main deployment plan
    â”œâ”€â”€ backup.pp        # Backup orchestration (PRD section 7)
    â”œâ”€â”€ update.pp        # Change management (PRD section 12)
    â””â”€â”€ dr_restore.pp    # Disaster recovery
```

### `/terraform/` - Infrastructure as Code
```
terraform/
â”œâ”€â”€ environments/        # Environment-specific configs
â”‚   â”œâ”€â”€ dev/            # Development environment
â”‚   â”œâ”€â”€ staging/        # Test environment (PRD section 9)
â”‚   â””â”€â”€ prod/           # Production environment
â”‚       â”œâ”€â”€ main.tf      # Main configuration
â”‚       â”œâ”€â”€ variables.tf # Input variables
â”‚       â”œâ”€â”€ outputs.tf   # Output values
â”‚       â””â”€â”€ terraform.tfvars.example
â””â”€â”€ modules/            # Reusable modules
    â”œâ”€â”€ puppet-infrastructure/  # Puppet setup
    â”œâ”€â”€ k3s-cluster/           # K3s deployment
    â”œâ”€â”€ data-platform/         # Data engineering stack
    â”œâ”€â”€ monitoring/            # Observability stack
    â”œâ”€â”€ security/              # Security components
    â””â”€â”€ networking/            # VLANs, DNS, VPN
```

### `/k8s/` - Kubernetes Manifests
```
k8s/
â”œâ”€â”€ base/               # Base manifests
â”‚   â”œâ”€â”€ namespaces/     # Namespace definitions
â”‚   â”œâ”€â”€ rbac/           # RBAC policies (PRD section 11)
â”‚   â””â”€â”€ network-policies/ # Security zones (PRD section 6)
â”œâ”€â”€ overlays/           # Environment overrides
â”‚   â””â”€â”€ prod/
â”‚       â”œâ”€â”€ data-platform/    # NiFi, Trino configs
â”‚       â”œâ”€â”€ monitoring/       # Prometheus, Grafana
â”‚       â””â”€â”€ kustomization.yaml
â”œâ”€â”€ applications/       # GitOps application definitions
â”‚   â”œâ”€â”€ nifi-app.yaml
â”‚   â”œâ”€â”€ trino-app.yaml
â”‚   â””â”€â”€ jupyterlab-app.yaml
â””â”€â”€ helm-values/        # Helm chart configurations
    â”œâ”€â”€ nifi-values.yaml
    â”œâ”€â”€ trino-values.yaml
    â””â”€â”€ minio-values.yaml
```

## Network Architecture (PRD Section 5)

### IP Allocation
```
# Management VLAN (10.0.0.x / 192.168.0.x)
192.168.0.120 - Pi Master (Puppet Server + K3s Master)
192.168.0.121 - Pi Worker 1 (Trino Workers)
192.168.0.122 - Pi Worker 2 (PostgreSQL + Data Storage)
192.168.0.123 - Pi Worker 3 (MinIO + Backup Storage)
192.168.0.200-250 - MetalLB IP Pool (LoadBalancer Services)

# Services VLAN (10.0.1.x) - Internal only
10.0.1.10 - NiFi Web UI
10.0.1.11 - Trino Coordinator
10.0.1.12 - Grafana Dashboard
10.0.1.13 - JupyterLab

# Pod Network (10.0.2.x) - K3s internal
10.0.2.0/24 - Pod CIDR
```

### DNS Configuration
```
# Add to your router or /etc/hosts:
192.168.0.120 puppet.cluster.local puppet
192.168.0.120 pi-master.cluster.local pi-master
192.168.0.121 pi-worker-1.cluster.local pi-worker-1
192.168.0.122 pi-worker-2.cluster.local pi-worker-2
192.168.0.123 pi-worker-3.cluster.local pi-worker-3

# Service DNS (handled by CoreDNS)
nifi.data-platform.svc.cluster.local
trino.data-platform.svc.cluster.local
minio.data-platform.svc.cluster.local
```

## Automation Flow

### Daily Operations (PRD Section 11 - Roles & Access)
```bash
# Admin Tasks
make cluster-status
make backup

# Developer Tasks
# Deploy NiFi flow
kubectl apply -f flows/my-nifi-flow.yaml

# Query data with Trino
kubectl exec -it trino-coordinator-0 -n data-platform -- trino
> SELECT * FROM hive.default.sensor_data WHERE date > CURRENT_DATE - INTERVAL '7' DAY;

# Access JupyterLab
kubectl port-forward -n data-platform svc/jupyterlab 8888:8888
# Browse to http://localhost:8888

# Guest Access (Read-only Grafana)
kubectl port-forward -n monitoring svc/grafana 3000:3000
# Browse to http://localhost:3000 (read-only user)
```

### Change Management (PRD Section 12)
```bash
# GitOps workflow
git checkout -b feature/update-nifi-flow
# Make changes
git add .
git commit -m "feat: update NiFi flow for weather data"
git push origin feature/update-nifi-flow
# Create PR, get approval, merge

# Flux CD automatically syncs (every 1 min)
flux get sources git
flux get kustomizations

# Manual sync if needed
flux reconcile source git flux-system
```

### Maintenance Tasks
```bash
# Monthly patching window
bolt plan run pi_cluster_automation::patch_nodes \
  --targets all -i inventory.yaml

# Backup before changes
make backup BACKUP_NAME=pre-patch-$(date +%Y%m%d)

# Rolling update
bolt plan run pi_cluster_automation::rolling_update \
  --targets all -i inventory.yaml
```

## Troubleshooting Guide

### Common Issues

#### 1. NiFi Connection Issues
```bash
# Check NiFi pods
kubectl logs -n data-platform deployment/nifi

# Verify certificates
kubectl get certificates -n data-platform

# Check persistent volumes
kubectl get pvc -n data-platform
```

#### 2. Trino Query Failures
```bash
# Check Trino coordinator
kubectl logs -n data-platform trino-coordinator-0

# Verify catalog configuration
kubectl exec -it trino-coordinator-0 -n data-platform -- cat /etc/trino/catalog/hive.properties

# Test connectivity to metastore
kubectl exec -it trino-coordinator-0 -n data-platform -- nc -zv postgres-service 5432
```

#### 3. Storage Issues
```bash
# Check MinIO
kubectl logs -n data-platform deployment/minio

# Verify storage capacity
df -h  # On each node

# Check Longhorn (if using)
kubectl get volumes -n longhorn-system
```

### Recovery Procedures

#### Data Recovery (PRD Section 7)
```bash
# Restore from Velero backup
velero restore create --from-backup daily-backup-20250610

# Restore PostgreSQL
kubectl exec -it postgres-0 -n data-platform -- pg_restore /backup/latest.dump

# Restore MinIO buckets
mc mirror --overwrite backup-nas/minio/ minio/
```

#### Complete Cluster Recovery
```bash
# From backup
./scripts/disaster-recovery/restore-cluster.sh latest prod

# Verify data integrity
./scripts/verify-data-integrity.sh
```

## Why This Architecture Works

1. **Meets All PRD Requirements**:
   - Complete data engineering platform (NiFi, Trino, PostgreSQL, MinIO)
   - Full monitoring and observability
   - Security at all layers
   - GitOps-based deployment

2. **Scalability**:
   - Easy to add more Pi nodes
   - Horizontal scaling for Trino and storage
   - Distributed workload management

3. **Resilience**:
   - Multiple backup strategies (Velero, rsync)
   - Automated failover with K3s
   - Disaster recovery procedures

4. **Cost-Effective**:
   - Low power consumption (~20W per Pi)
   - No licensing costs (all open source)
   - Learn enterprise patterns at home

5. **Educational Value**:
   - Hands-on experience with modern DevOps
   - Real data engineering workflows
   - Production-like environment

## âœ… Conclusion

This architecture provides a **complete implementation** of enterprise-grade infrastructure patterns, delivering a professional data engineering platform on Raspberry Pi hardware.

### ðŸ† **What You Get**
- **Enterprise-Grade Infrastructure**: Production patterns on affordable hardware
- **Complete Automation**: 26 PowerShell commands + 23 Puppet tasks
- **Validated Deployment**: 98.7% infrastructure readiness with comprehensive testing
- **Real Data Engineering**: NiFi, Trino, PostgreSQL, MinIO stack fully operational
- **Comprehensive Monitoring**: Prometheus, Grafana, AlertManager with custom dashboards
- **Security Best Practices**: RBAC, network policies, secrets management
- **Educational Value**: Learn modern DevOps on real, working infrastructure

### ðŸŽ¯ **Perfect for**
- **DevOps Engineers**: Learn Infrastructure as Code and automation
- **Data Engineers**: Practice with enterprise data tools
- **Students & Educators**: Hands-on experience with modern tech stack
- **Home Lab Enthusiasts**: Professional infrastructure at home
- **Enterprise Teams**: Prototype and validate patterns before production

### ðŸ’¡ **Why This Matters**
This isn't just a "toy" cluster - it's a **complete enterprise data platform** that demonstrates:
- Modern Infrastructure as Code practices
- Production-ready monitoring and observability
- Enterprise data engineering workflows
- Comprehensive automation and GitOps
- Security and compliance best practices
- Cost-effective learning environment

**Start building your enterprise data platform today!** ðŸš€
